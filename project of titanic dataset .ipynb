{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd# data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom matplotlib import style\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data=pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ndata['Age']=data[\"Age\"].fillna(data['Age'].mean())\nprint(data.describe())\nprint(data.head)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets get the visualisation of the data by plotting different graphs and figures\ndata['dead']=1-data['Survived']\ndata.groupby('Sex').mean()[['Survived','dead']].plot(kind='bar',stacked=True,colors=['g','r'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig=plt.figure(figsize=(25,7))\nsns.violinplot(x='Sex',y='Age',hue='Survived',data=data,split=True,palette={0:'r',1:'g'})\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"figure=plt.figure(figsize=(25,7))\nplt.hist([data[data['Survived']==1]['Fare'],data[data['Survived']==0]['Fare']]\n         ,stacked=True,color=['g','r'],\n         bins=50,label=['Survived','dead'])\nplt.xlabel('fare')\nplt.ylabel('No/. of passenger')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## lets combine the age survived and fare\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(22,7))\nax=plt.subplot()\nax.scatter(data[data['Survived']==1]['Age'],data[data['Survived']==1]['Fare'],c='green',s=data[data['Survived']==1]['Fare'])\nax.scatter(data[data['Survived']==0]['Age'],data[data['Survived']==0]['Fare'],c='red',s=data[data['Survived']==0]['Fare'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fare corelates with the class below","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax=plt.subplot()\nax.set_ylabel('Average fare')\ndata.groupby('Pclass').mean()['Fare'].plot(kind='bar',figsize=(25,7))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#similarly see the embarkation sideeffect ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig=plt.figure()\nsns.violinplot(x='Embarked',y='Fare',hue='Survived',data=data,split=True,palette={0:'r',1:'g'})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Feature Engineering**********\nextracting the passengers title"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_combined_data():\n    train=pd.read_csv(\"/kaggle/input/titanic/train.csv\")\n    test=pd.read_csv(\"/kaggle/input/titanic/test.csv\")\n    targets=train.Survived\n    train.drop(['Survived'],1,inplace=True)\n    combined=train.append(test)\n    combined.reset_index(inplace=True)\n  \n    return combined\ncombined=get_combined_data()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"titles=set()\nfor name in data['Name']:\n    titles.add(name.split(',')[1].split('.')[0].strip())\ntitle_dict={\n    \"Lady\":\"Royality\",\n    \"Capt\": \"Officer\",\n    \"Col\": \"Officer\",\n    \"Major\": \"Officer\",\n    \"Jonkheer\": \"Royalty\",\n    \"Don\": \"Royalty\",\n    \"Sir\" : \"Royalty\",\n    \"Dr\": \"Officer\",\n    \"Rev\": \"Officer\",\n    \"the Countess\":\"Royalty\",\n    \"Mme\": \"Mrs\",\n    \"Mlle\": \"Miss\",\n    \"Ms\": \"Mrs\",\n    \"Mr\" : \"Mr\",\n    \"Mrs\" : \"Mrs\",\n    \"Miss\" : \"Miss\",\n    \"Master\" : \"Master\"\n           }\ndef get_titles():\n    combined['Title']=combined['Name'].map(lambda name: name.split(',')[1].split('.')[0].strip())\n    combined['Title']=combined.Title.map(title_dict)\n\n    return combined\ncombined=get_titles()\nprint(combined.head)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Processing Ages\n#print(combined.iloc[:891].Age.isnull().sum())\n#print(combined.iloc[891:].Age.isnull().sum())\ngrouped_train=combined.iloc[:891].groupby(['Sex','Pclass','Title'])\ngrouped_train_median=grouped_train.median()\ngrouped_train_median=grouped_train_median.reset_index()[['Sex','Pclass','Title','Age']]\nprint(grouped_train_median.head())\ndef fillage(row):\n    condition=(grouped_train_median['Sex']==row['Sex'])&(grouped_train_median['Title']==row['Title'])&(grouped_train_median['Pclass']==row['Pclass'])\n    return grouped_train_median[condition]['Age'].values[0]\ndef process_age():\n    global combined\n    combined['Age']=combined.apply(lambda row:fillage(row) if np.isnan(row['Age']) else row['Age'],axis=1)\n    return combined\ncombined=process_age()\nprint(combined['Age'].isnull)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" **Prcoess name**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate_name():\n    global combined\n    combined.drop('Name',axis=1,inplace=True)\n    dummies=pd.get_dummies(combined['Title'],prefix='Title')\n    combined=pd.concat([combined,dummies],axis=1)\n    combined.drop('Title',inplace=True,axis=1)\n    return combined\ncombined=evaluate_name()\nprint(combined.head())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**process Sex**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def processsex():\n    global combined\n    combined['Sex']=combined['Sex'].map({'male':1,'female':0})\n    return combined\ncombined=processsex()\nprint(combined.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Process Fare**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def Processfare():\n    global combined\n    combined.Fare.fillna(combined.iloc[:891].Fare.mean(),inplace=True)\n    return combined\ncombined=Processfare()\nprint(combined['Fare'].isnull)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Processing Embarked**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def Process_embarked():\n    global combined\n    combined.Embarked.fillna('S',inplace=True)\n    dummies=pd.get_dummies(combined['Embarked'],prefix='embark')\n    combined=pd.concat([combined,dummies],axis=1)\n    combined.drop('Embarked',axis=1,inplace=True)\n    return combined\ncombined=Process_embarked()\nprint(combined.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**processing cabin******"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_cabin,test_cabin=set(),set()\nfor c in combined.iloc[:891]['Cabin']:\n    try:\n        train_cabin.add(c[0])\n    except:\n        train_cabin.add('U')\nfor c in combined.iloc[891:]['Cabin']:\n    try:\n        test_cabin.add(c[0])\n    except:\n        test_cabin.add('U')        \nprint(train_cabin)\nprint(test_cabin)\ndef processcabin():\n    global combined\n    combined.Cabin.fillna('U',inplace=True)\n    combined['Cabin']=combined['Cabin'].map(lambda c:c[0])\n    dummy=pd.get_dummies(combined['Cabin'],prefix='Cabin')\n    combined=pd.concat([combined,dummy],axis=1)\n    combined.drop('Cabin',axis=1,inplace=True)\n    return combined\ncombined=processcabin()\nprint(combined.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Processing Pclasss**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def processingPclass():\n    global combined\n    dummy=pd.get_dummies(combined['Pclass'],prefix='Class')\n    combined.drop('Pclass',axis=1,inplace=True)\n    combined=pd.concat([combined,dummy],axis=1)\n    return combined\ncombined=processingPclass()\nprint(combined.head())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Processing Ticket**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def filterticket(ticket):\n    ticket=ticket.replace('/','')\n    ticket=ticket.replace('.','')\n    ticket=ticket.split()\n    ticket=map(lambda t: t.strip(), ticket)\n    ticket=list(filter(lambda t:  not t.isdigit(),ticket))\n    if (len(ticket)>0):\n        return ticket[0]\n    else:\n        return 'XXX'\ntickets=set()    \nfor t in combined['Ticket']:\n    tickets.add(filterticket(t))\nprint(tickets)    \ndef processtickets():\n    global combined\n    def filterticket(ticket):\n        ticket=ticket.replace('/','')\n        ticket=ticket.replace('.','')\n        ticket=ticket.split()\n        ticket=map(lambda t: t.strip(), ticket)\n        ticket=list(filter(lambda t:  not t.isdigit(),ticket))\n        if (len(ticket)>0):\n            return ticket[0]\n        else:\n            return 'XXX' \n    combined['Ticket']=combined['Ticket'].map(filterticket)\n    dummy=pd.get_dummies(combined['Ticket'],prefix='Ticket')\n\n    combined.drop('Ticket',axis=1,inplace=True)\n    combined=pd.concat([combined,dummy],axis=1)\n    return combined\ncombined=processtickets()\nprint(combined.head())\n    \n    \n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Process Family"},{"metadata":{"trusted":true},"cell_type":"code","source":"def Processfamily():\n    global combined\n    combined['Familysize']=combined['SibSp']+combined['Parch']+1\n    combined['Single']=combined['Familysize'].map(lambda s: 1 if s==1 else 0)\n    combined['Small']=combined['Familysize'].map(lambda s: 1 if s>=2 and s<=4 else 0)\n    combined['big']=combined['Familysize'].map(lambda s: 1 if s>4  else 0)\n    return combined\ncombined=Processfamily()\ncombined.drop('SibSp',axis=1,inplace=True)\ncombined.drop('Parch',axis=1,inplace=True)\nprint(combined.head())\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1. breaking the train and test from combined data\ndef recovery():\n    global combined\n    target=pd.read_csv(\"/kaggle/input/titanic/train.csv\",usecols=['Survived'])['Survived'].values\n    train=combined.iloc[:891]\n    test=combined.iloc[891:]\n    return train,test,target\ntrain,test,target=recovery()\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#2 we import all the necessary libraries\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble.gradient_boosting import GradientBoostingClassifier\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.linear_model import LogisticRegression,LogisticRegressionCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf=RandomForestClassifier(n_estimators=50,max_features='sqrt')\nclf=clf.fit(train,target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features=pd.DataFrame()\nfeatures['feature']=train.columns\nfeatures['importance']=clf.feature_importances_\nfeatures.sort_values(by=['importance'],ascending=True,inplace=True)\nfeatures.set_index('feature',inplace=True)\nfeatures.plot(kind='barh',figsize=(25,35))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model=SelectFromModel(clf,prefit=True)\ntrain_reduced=model.transform(train)\nprint(train_reduced.shape)\ntest_reduced=model.transform(test)\nprint(test_reduced.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#3 lets try different models\ndef compute_score(clf,x,y,scoring='accuracy'):\n    xval=cross_val_score(clf,x,y,cv=5,scoring=scoring)\n    return np.mean(xval)\n\nlogreg=LogisticRegression()\nlogregcv=LogisticRegressionCV()\nrf=RandomForestClassifier()\ngboost=GradientBoostingClassifier()\nmodels=[logreg,logregcv,rf,gboost]\nfor model in models:\n    print(\"cross validation of:{0}\".format(model.__class__))\n    score=compute_score(clf=model,x=train_reduced,y=target,scoring='accuracy')\n    print(\"cv score{0}\".format(score))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#turn run_gs to True if you want to run the gridsearch again.\nrun_gs = False\n\nif run_gs:\n    parameter_grid = {\n                 'max_depth' : [4, 6, 8],\n                 'n_estimators': [50, 10],\n                 'max_features': ['sqrt', 'auto', 'log2'],\n                 'min_samples_split': [2, 3, 10],\n                 'min_samples_leaf': [1, 3, 10],\n                 'bootstrap': [True, False],\n                 }\n    forest = RandomForestClassifier()\n    cross_validation = StratifiedKFold(n_splits=5)\n\n    grid_search = GridSearchCV(forest,\n                               scoring='accuracy',\n                               param_grid=parameter_grid,\n                               cv=cross_validation,\n                               verbose=1\n                              )\n\n    grid_search.fit(train, target)\n    model = grid_search\n    parameters = grid_search.best_params_\n\n    print('Best score: {}'.format(grid_search.best_score_))\n    print('Best parameters: {}'.format(grid_search.best_params_))\n    \nelse: \n    parameters = {'bootstrap': False, 'min_samples_leaf': 3, 'n_estimators': 50, \n                  'min_samples_split': 10, 'max_features': 'sqrt', 'max_depth': 6}\n    \n    model = RandomForestClassifier(**parameters)\n    model.fit(train, target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output = model.predict(test).astype(int)\ndf_output = pd.DataFrame()\naux = pd.read_csv('/kaggle/input/titanic/test.csv')\ndf_output['PassengerId'] = aux['PassengerId']\ndf_output['Survived'] = output\ndf_output[['PassengerId','Survived']].to_csv('submission_test.csv', index=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"check_submission = pd.read_csv('submission_test.csv')\nprint(check_submission)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}